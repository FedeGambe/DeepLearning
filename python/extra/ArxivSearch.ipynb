{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def search(topic: str): \n",
    "    url = \"http://export.arxiv.org/api/query\"\n",
    "    params = {\n",
    "        \"search_query\": f\"all:{topic}\",\n",
    "        \"sortBy\": \"submittedDate\",\n",
    "        \"sortOrder\": \"descending\",\n",
    "        \"max_results\": 5\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3Afederate%20learning%26id_list%3D%26start%3D0%26max_results%3D5\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=all:federate learning&amp;id_list=&amp;start=0&amp;max_results=5</title>\\n  <id>http://arxiv.org/api/SgXP0LUmTkLB86K8VmcAld0SACc</id>\\n  <updated>2024-02-19T00:00:00-05:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">294333</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">5</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2402.10898v1</id>\\n    <updated>2024-02-16T18:56:41Z</updated>\\n    <published>2024-02-16T18:56:41Z</published>\\n    <title>The Price of Adaptivity in Stochastic Convex Optimization</title>\\n    <summary>  We prove impossibility results for adaptivity in non-smooth stochastic convex\\noptimization. Given a set of problem parameters we wish to adapt to, we define\\na \"price of adaptivity\" (PoA) that, roughly speaking, measures the\\nmultiplicative increase in suboptimality due to uncertainty in these\\nparameters. When the initial distance to the optimum is unknown but a gradient\\nnorm bound is known, we show that the PoA is at least logarithmic for expected\\nsuboptimality, and double-logarithmic for median suboptimality. When there is\\nuncertainty in both distance and gradient norm, we show that the PoA must be\\npolynomial in the level of uncertainty. Our lower bounds nearly match existing\\nupper bounds, and establish that there is no parameter-free lunch.\\n</summary>\\n    <author>\\n      <name>Yair Carmon</name>\\n    </author>\\n    <author>\\n      <name>Oliver Hinder</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2402.10898v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10898v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2402.10894v1</id>\\n    <updated>2024-02-16T18:51:42Z</updated>\\n    <published>2024-02-16T18:51:42Z</published>\\n    <title>Fusion of Diffusion Weighted MRI and Clinical Data for Predicting\\n  Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning</title>\\n    <summary>  Stroke is a common disabling neurological condition that affects about\\none-quarter of the adult population over age 25; more than half of patients\\nstill have poor outcomes, such as permanent functional dependence or even\\ndeath, after the onset of acute stroke. The aim of this study is to investigate\\nthe efficacy of diffusion-weighted MRI modalities combining with structured\\nhealth profile on predicting the functional outcome to facilitate early\\nintervention. A deep fusion learning network is proposed with two-stage\\ntraining: the first stage focuses on cross-modality representation learning and\\nthe second stage on classification. Supervised contrastive learning is\\nexploited to learn discriminative features that separate the two classes of\\npatients from embeddings of individual modalities and from the fused multimodal\\nembedding. The network takes as the input DWI and ADC images, and structured\\nhealth profile data. The outcome is the prediction of the patient needing\\nlong-term care at 3 months after the onset of stroke. Trained and evaluated\\nwith a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80\\nand 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing\\nmodels that consolidate both imaging and structured data in the medical domain.\\nIf trained with comprehensive clinical variables, including NIHSS and\\ncomorbidities, the gain from images on making accurate prediction is not\\nconsidered substantial, but significant. However, diffusion-weighted MRI can\\nreplace NIHSS to achieve comparable level of accuracy combining with other\\nreadily available clinical variables for better generalization.\\n</summary>\\n    <author>\\n      <name>Chia-Ling Tsai</name>\\n    </author>\\n    <author>\\n      <name>Hui-Yun Su</name>\\n    </author>\\n    <author>\\n      <name>Shen-Feng Sung</name>\\n    </author>\\n    <author>\\n      <name>Wei-Yang Lin</name>\\n    </author>\\n    <author>\\n      <name>Ying-Ying Su</name>\\n    </author>\\n    <author>\\n      <name>Tzu-Hsien Yang</name>\\n    </author>\\n    <author>\\n      <name>Man-Lin Mai</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 5 figures, 5 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2402.10894v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10894v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2402.10893v1</id>\\n    <updated>2024-02-16T18:50:24Z</updated>\\n    <published>2024-02-16T18:50:24Z</published>\\n    <title>RLVF: Learning from Verbal Feedback without Overgeneralization</title>\\n    <summary>  The diversity of contexts in which large language models (LLMs) are deployed\\nrequires the ability to modify or customize default model behaviors to\\nincorporate nuanced requirements and preferences. A convenient interface to\\nspecify such model adjustments is high-level verbal feedback, such as \"Don\\'t\\nuse emojis when drafting emails to my boss.\" However, while writing high-level\\nfeedback is far simpler than collecting annotations for reinforcement learning\\nfrom human feedback (RLHF), we find that simply prompting a model with such\\nfeedback leads to overgeneralization of the feedback to contexts where it is\\nnot relevant. We study the problem of incorporating verbal feedback without\\nsuch overgeneralization, inspiring a new method Contextualized Critiques with\\nConstrained Preference Optimization (C3PO). C3PO uses a piece of high-level\\nfeedback to generate a small synthetic preference dataset specifying how the\\nfeedback should (and should not) be applied. It then fine-tunes the model in\\naccordance with the synthetic preference data while minimizing the divergence\\nfrom the original model for prompts where the feedback does not apply. Our\\nexperimental results indicate that our approach effectively applies verbal\\nfeedback to relevant scenarios while preserving existing behaviors for other\\ncontexts. For both human- and GPT-4-generated high-level feedback, C3PO\\neffectively adheres to the given feedback comparably to in-context baselines\\nwhile reducing overgeneralization by 30%.\\n</summary>\\n    <author>\\n      <name>Moritz Stephan</name>\\n    </author>\\n    <author>\\n      <name>Alexander Khazatsky</name>\\n    </author>\\n    <author>\\n      <name>Eric Mitchell</name>\\n    </author>\\n    <author>\\n      <name>Annie S Chen</name>\\n    </author>\\n    <author>\\n      <name>Sheryl Hsu</name>\\n    </author>\\n    <author>\\n      <name>Archit Sharma</name>\\n    </author>\\n    <author>\\n      <name>Chelsea Finn</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2402.10893v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10893v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2402.10892v1</id>\\n    <updated>2024-02-16T18:49:27Z</updated>\\n    <published>2024-02-16T18:49:27Z</published>\\n    <title>Proving membership in LLM pretraining data via data watermarks</title>\\n    <summary>  Detecting whether copyright holders\\' works were used in LLM pretraining is\\npoised to be an important problem. This work proposes using data watermarks to\\nenable principled detection with only black-box model access, provided that the\\nrightholder contributed multiple training documents and watermarked them before\\npublic release. By applying a randomly sampled data watermark, detection can be\\nframed as hypothesis testing, which provides guarantees on the false detection\\nrate. We study two watermarks: one that inserts random sequences, and another\\nthat randomly substitutes characters with Unicode lookalikes. We first show how\\nthree aspects of watermark design -- watermark length, number of duplications,\\nand interference -- affect the power of the hypothesis test. Next, we study how\\na watermark\\'s detection strength changes under model and dataset scaling: while\\nincreasing the dataset size decreases the strength of the watermark, watermarks\\nremain strong if the model size also increases. Finally, we view SHA hashes as\\nnatural watermarks and show that we can robustly detect hashes from\\nBLOOM-176B\\'s training data, as long as they occurred at least 90 times.\\nTogether, our results point towards a promising future for data watermarks in\\nreal world use.\\n</summary>\\n    <author>\\n      <name>Johnny Tian-Zheng Wei</name>\\n    </author>\\n    <author>\\n      <name>Ryan Yixiang Wang</name>\\n    </author>\\n    <author>\\n      <name>Robin Jia</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2402.10892v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10892v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2402.10891v1</id>\\n    <updated>2024-02-16T18:47:21Z</updated>\\n    <published>2024-02-16T18:47:21Z</published>\\n    <title>Instruction Diversity Drives Generalization To Unseen Tasks</title>\\n    <summary>  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\\ninstructions and desired outcomes -- is an approach that enables pre-trained\\nlanguage models to perform real-world tasks and follow human instructions. Its\\npractical success depends on the model learning a broader set of instructions\\nthan those it was trained on. Yet the factors that determine model\\ngeneralization to such \\\\emph{unseen tasks} are not well understood. %To\\nunderstand the driving factors of generalization, In this paper, we experiment\\nwith string rewrites, a symbolic task that serves as a building block for\\nTuring complete Markov algorithms while allowing experimental control of\\n\"inputs\" and \"instructions\". We investigate the trade-off between the number of\\ninstructions the model is trained on and the number of training samples\\nprovided for each instruction and observe that the diversity of the instruction\\nset determines generalization. Generalization emerges once a diverse enough set\\nof tasks is provided, even though very few examples are provided for each task.\\nInstruction diversity also ensures robustness with respect to non-uniform\\ndistributions of instructions in the training set.\\n</summary>\\n    <author>\\n      <name>Dylan Zhang</name>\\n    </author>\\n    <author>\\n      <name>Justin Wang</name>\\n    </author>\\n    <author>\\n      <name>Francois Charton</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2402.10891v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10891v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = \"federate learning\"\n",
    "search(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arxiv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "                    query = \"Federate Learning\",\n",
    "                    max_results = 3,\n",
    "                    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "                    )\n",
    "\n",
    "results = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Price of Adaptivity in Stochastic Convex Optimization\n",
      "Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning\n",
      "RLVF: Learning from Verbal Feedback without Overgeneralization\n"
     ]
    }
   ],
   "source": [
    "# `results` is a generator; you can iterate over its elements one by one...\n",
    "for r in client.results(search):\n",
    "    print(r.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Price of Adaptivity in Stochastic Convex Optimization', 'Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning', 'RLVF: Learning from Verbal Feedback without Overgeneralization']\n"
     ]
    }
   ],
   "source": [
    "# ...or exhaust it into a list. Careful: this is slow for large results sets.\n",
    "all_results = list(results)\n",
    "print([r.title for r in all_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/cond-mat/0603029v1\n"
     ]
    }
   ],
   "source": [
    "# For advanced query syntax documentation, see the arXiv API User Manual:\n",
    "# https://arxiv.org/help/api/user-manual#query_details\n",
    "search = arxiv.Search(query = \"au:del_maestro AND ti:checkerboard\")\n",
    "first_result = next(client.results(search))\n",
    "print(first_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From stripe to checkerboard order on the square lattice in the presence of quenched disorder\n"
     ]
    }
   ],
   "source": [
    "# Search for the paper with ID \"1605.08386v1\"\n",
    "search_by_id = arxiv.Search(id_list=[\"1605.08386v1\"])\n",
    "# Reuse client to fetch the paper, then print its title.\n",
    "first_result = next(client.results(search))\n",
    "print(first_result.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./1605.08386v1.Heat_bath_random_walks_with_Markov_bases.pdf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "paper = [\"1605.08386v1\"]\n",
    "retrieve_paper = next(arxiv.Client().results(arxiv.Search(id_list=paper)))\n",
    "# Download the PDF to the PWD with a default filename.\n",
    "retrieve_paper.download_pdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mydir\\\\1605.08386v1.pdf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the PDF to a specified directory with a custom filename.\n",
    "retrieve_paper.download_pdf(dirpath=\"./mydir\", filename=f\"{paper[0]}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
